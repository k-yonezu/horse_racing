{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pickle\n",
    "import utils.read_data as rd\n",
    "import utils.preprocessing as pp\n",
    "import utils.join_race_data as jrd\n",
    "import utils.prepare_data as prepare_data\n",
    "\n",
    "import utils.io_model as im\n",
    "import os\n",
    "from os.path import join, dirname\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(verbose=True)\n",
    "dotenv_path = join(Path().resolve(), '.env')\n",
    "load_dotenv(dotenv_path)\n",
    "GOOGLE_DRIVE_PATH = os.environ.get(\"GOOGLE_DRIVE_PATH\")\n",
    "TRAIN_DATA_PATH = GOOGLE_DRIVE_PATH + '/train_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ジャパンカップ当日の別レースデータをこのノートブックで使えるようにディレクトリに配置\n",
    "import shutil\n",
    "\n",
    "target_file_names = [name for name in os.listdir(DATA_PATH+\"test_data\") if not \"japan\" in name and  not \"arima\" in name]\n",
    "race_files = sorted([name for name in target_file_names if \"race\" in name])\n",
    "horse_files = sorted([name for name in target_file_names if \"horse\" in name])\n",
    "race_names = [race_name.split(\"-\")[1] for race_name in race_files]\n",
    "race_names = [race_name.split(\".\")[0] for race_name in race_files]\n",
    "for race_name, race_file, horse_file in zip(race_names, race_files, horse_files):\n",
    "    target_data_path = DATA_PATH+\"test_data/target_\"+race_name\n",
    "    os.makedirs(target_data_path, exist_ok=True)\n",
    "    shutil.move(DATA_PATH+\"test_data/\"+race_file, target_data_path+\"/\")\n",
    "    shutil.move(DATA_PATH+\"test_data/\"+horse_file, target_data_path+\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TARGET_DATA_PATH = GOOGLE_DRIVE_PATH + '/csv/test_data/target_race-202005050902/'\n",
    "TARGET_DATA_PATH = GOOGLE_DRIVE_PATH + '/test_data/takarazuka/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target_horse = rd.read_target_horse_csv(TARGET_DATA_PATH)\n",
    "df_target_race  = rd.read_target_race_csv(TARGET_DATA_PATH)\n",
    "target_df = pd.merge(df_target_horse, df_target_race, on='race_id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ターゲットデータに過去3レース分の情報を追加\n",
    "past_data_df = rd.read_horse_race_csv(TRAIN_DATA_PATH)\n",
    "columns_past_data = [c for c in past_data_df.columns if \"-\" not in c]\n",
    "df_for_prediction = jrd.join_n_race_for_test_data(past_data_df[columns_past_data], target_df, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_label(rank):\n",
    "    rank = str(rank)\n",
    "    if not(rank.isdigit()):\n",
    "        rank = 30\n",
    "\n",
    "    return int(rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_for_prediction = df_for_prediction.rename(columns={'total_horse_number': 'total_horse_number_x'})\n",
    "#df_for_prediction['rank'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_prediction[\"rank-1\"] = df_for_prediction[\"rank-1\"].apply(make_label)\n",
    "df_for_prediction[\"rank-2\"] = df_for_prediction[\"rank-2\"].apply(make_label)\n",
    "df_for_prediction[\"rank-3\"] = df_for_prediction[\"rank-3\"].apply(make_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "past_data_df[\"rank-1\"] = past_data_df[\"rank-1\"].apply(make_label)\n",
    "past_data_df[\"rank-2\"] = past_data_df[\"rank-2\"].apply(make_label)\n",
    "past_data_df[\"rank-3\"] = past_data_df[\"rank-3\"].apply(make_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_for_prediction = prepare_data.prepare_data_for_prediction(df_for_prediction, past_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(df_for_prediction.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df_for_prediction.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = './model_data/lambdarank/lgb_model.pkl'\n",
    "model = pickle.load(open(file, 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(df_for_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict(x, num_iteration=model.best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = pd.DataFrame(columns=['horse_id', 'horse_number', 'score'])\n",
    "for score, horse_number, horse_id in zip(pred, list(target_df[\"horse_number\"]), list(target_df[\"horse_id\"])):\n",
    "    res = res.append(pd.DataFrame([[horse_id, horse_number, -score]], columns=['horse_id', 'horse_number', 'score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(a):\n",
    "    x = np.exp(a)\n",
    "    u = np.sum(x)\n",
    "    return x/u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res['proc'] = softmax(np.array(res[\"score\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(res.proc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.sort_values('score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
